\documentclass[man,floatsintext]{apa6}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{times}

\shorttitle{Semantic coherence and distributional learning}
\title{Semantic coherence facilitates distributional learning}
\author{Long Ouyang, Lera Boroditsky, Michael C. Frank}
\affiliation{Stanford University}
\authornote{Corresponding author:\\
Long Ouyang\\
louyang@post.harvard.edu
}
\abstract{Abstract here.}

\begin{document}
\maketitle

How do people learn the meanings of words? Quine (1960) has famously observed that, based on the evidence we receive, any given word has an infinite number of logically consistent meanings. Suppose, Quine says, that we observe a foreigner utter the word \emph{gavagai} upon seeing a rabbit. There are too many consistent meanings, including ``food'', ``let's go hunting'', ``a momentary stage-rabbit'', ``undetached rabbit parts'', and so on.

One way that the hard problem of learning is softened is by a multiplicity of sources of information about word meaning. The external world, other people, and language itself provide various sources that the aspirant learner can use. Examples of informative sources include vision (Kay \& McDaniel, 1978; Rescorla, 1980; Booth \& Waxman, 2002; Regier \& Zheng, 2007), social experience (Baldwin, 1991; Bloom, 2002; Tomasello, 2003), phonology (Bloomfield, 1895; Kohler, 1929; Ramachandran and Hubbard, 2001; Maurer, Pathman, \& Mondloch, 2006; Graf-Estes et al., 2007; Parault \& Parkinson, 2008; Shukla, White, \& Aslin, 2011), syntax (Gleitman, 1990; Fisher et al., 1994), morphology (Carr \& Johnston, 2001), and linguistic context. Our chief interest in this paper is this last source -- linguistic context.

- It has long been believed that the linguistic contexts a word occurs in may be a useful clue toward its meaning. TODO: define the term distributional learning.

- While researchers believe that distributional learning is a powerful learning mechanism, empirical results using artificial language learning suggest that human capacity for distributional learning may be quite limited.
(what is the evidence that people are good at distributional learning?
is our finding on semantic coherence in tension with this earlier evidence?)
 
- In this paper, we help characterize some of the conditions under which distributional learning may succeed. Experimental work on distributional learning typically exposes learners to artificial languages where all the words are \emph{novel}. Our experiments suggest that distributional learning of meaning is facilitated by semantic coherence, the presence of \emph{known} words adhering to some semantic organization.

- To preview our paper: first, we review some of the general evidence for distributional learning. We then introduce the specific language structure we expolore in this paper, the MNPQ language, and discuss some of the failures to find evidence of distributional learning in experiments using this language. In Experiment 1, we present evidence that semantic coherence can facilitate MNPQ learning. In Experiments 2 and 3, we further explore this effect by isolate each component of semantic coherence -- meaning and coherence -- and find that neither meaning alone nor coherence alone are sufficient to facilitate MNPQ learning. We conclude with some remarks on potential mechanisms and limitations of purely artificial language learning.

\subsection{Evidence for distributional learning}

- philosophy/linguistics. wittgenstein, firth, harris.

- (transition: ideas from structural linguistics revived by computer scientists) computational models. HAL, LSA (diagrams), topic models. (bottom line: computational proof of concept)

- (transition: some suggestion of evidence for computational proofs) blind arguments.

- (transition: however, little direct evidence) while distributional learning has been studied empirically for syntax, not so much for semantics.

\subsection{A puzzle: the MNPQ language}

failures: Braine (1966), Smith, (1966); Braine, (1987); Frank \& Gibson (2011)

other references? brooks et al., 1993; frigo \& mcdonald, 1998; kempe \& brooks, 2001

mention work on correlated cues.

MNPQ is a simple case of inferring lexical categories from distributional statistics.

\section{Experiment 1: Semantic coherence}

\section{Experiment 2: Phonological coherence}

\section{Experiment 3: Semantic incoherence}

\section{General Discussion}

much overlap between DL of syntax and DL of semantics

\subsection{Mechanism: memory and what else?}

\subsection{Limitations of artificial languages} 
blah blah blah \cite{Imai:2008ee} meh

other blah blah \cite{Foss:1966jl}

\nocite{*}
\newpage
\bibliographystyle{apacite}
\bibliography{references}
\end{document}